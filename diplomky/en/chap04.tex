\chapter{Classifiers}
\label{chap:clscon}

\section{Naive Bayes}
Probabilistic model for classification
Generally speaking, there are two models used for classification; generative and discriminative. Generative model assumes there exist 

Naive Bayes- WARN: document vs instance mismatch
Also this is only with word features
There are two classification algorithms commonly referred to as naive Bayes. However, we will discuss them at once, because the underlying idea is the same for both.
Naive Bayes uses generative model assuming each instance was generated by a parametric model. Because the model is not know, NB estimates the probability of an instance being generated by a class instead. We assume every document d\_i is generated by a mixture model parameterized by theta. The mixture model consists of components {c\_1, ...c\_n} component C\_i generating class c\_i. Thus a document is generated by first picking a model component (P[c\_i|theta]) and then being generated by that component P(P[d\_i|c\_i, Theta]).
Mathematically speaking, probability of d\_i being generated is:
P[d\_i | Theta] = sum over j = P[d\_i|c\_j, Theta] P[c\_j|Theta]


It  does so by maximum likelihood estimation (MLE).
MLE is a statistical method for estimating parameters such that observed (training instances in this case) is the most probable. For discrete distribution it equals to counting frequencies.
 
For classification one simply assesses the probability of each class to generate a classified instance and classify the instance asm the class which had the highest probability.

\section{Decision trees}
\todoC{Decision trees}

\section{FastText}


\section{\todoC{NN}}

\section{\todoC{Let you imagination go wild}}
