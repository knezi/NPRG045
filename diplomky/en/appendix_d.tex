\chapter{Package Technicalities}\label{app:techn}

These parts are executed with make\footnote{url{https://www.gnu.org/software/make}} with dependencies properly set.
The entire project can be therefore run simply by \texttt{make run}.

The entire project runs in Python of version at least 3.6\footnote{\url{https://python.org}} and bash\footnote{\url{https://gnu.org/software/bash}}.
The following libraries are used:

\begin{itemize}
\item nltk
\item pandas
\item numpy
\item yaml
\item scipy
\item geneea-nlp-client
\item gensim
\end{itemize}

A new folder for statistics is created every run to prevent accidental overwriting of already obtained results.

This part itself can be run by:

\begin{code}
make data/data.json
\end{code}


Preparation is executed by:

\begin{code}
./denormalization/denormalize.sh ../data/dataset data/data.json data/ids
\end{code}

The first argument is a directory with extracted archive of the Yelp Dataset.
The second argument is a path to the instance file and the last one to the intermediate file.
The script itself filters out not-needed reviews.
Also, it adds the information from the spell checker.
Subsequently, it joins reviews and business by replacing business id by the actual data.
The result is written in the instance file.
Finally, it prepares list of all IDs of used reviews into the intermediate file one ID per line.
The exact description of data and files format can be found in \autoref{app:a}.

The Geneea analysis takes the text of the reviews and computes some additional linguistics data.
It is done on a separate server for performance reasons.
The geneea file is created and transferred to the local computer.
Because of this, make only notifies users when it is needed to supply a new version of the geneea file.
The resulting file must be named \texttt{data/geneea.json}.

And can be run by:

\begin{code}
make run
\end{code}

\section{Experiments File}

This YAML\footnote{\url{https://yaml.org}}  file contains entire configuration of experiments conducted.
It lists classification tasks and defines evaluation.
Each task is a definition of a classifier.
Evaluation element is a definition what properties should be plotted in a graph.

Experiments file has the following format.
The root element is a dictionary with three parts;
\texttt{config}, \texttt{tasks} and \texttt{graphs}.
An example file can be found bellow.

\begin{code}
config:
  chunks: 10
tasks:
  - name: 'zero-R'
    classificator: 'baseline'
    features:
      - REVIEWLEN
      - UNIGRAMS
    preprocessing:
      - 'mutualinformation'
      - 'featurematrixconversion'
    extra_data: []
    config:
      algorithm: 'zero-R'
      features_to_select: 2
graphs:
  - name: 'baseline'
    data:
      zero-R:
        - 'f-measure'
\end{code}

The config section is another dictionary.
It has only one element -- \texttt{chunks}.
It specifies the argument~$k$~for k-fold cross-validation.

The section \texttt{experiments} is a list of classification tasks.
Each element is a dictionary specifying the exact parameters.
Descriptions of individual fields is in \autoref{tab:exp_dict}.
The list \texttt{features} consists of names as defined in FeatureSetEnum.
The exact definition can be found in \autoref{tab:fea_grp}.
\texttt{extra\_data} is any property of instances available in the raw data.
An example is the original text or business attributes.

\begin{table}[h]

\centering
\begin{tabular}{lll}
\toprule
\textbf{element name} & \textbf{type} & \textbf{meaning}\\
\midrule
name 			& string	& identification referred to in graphs\\
classificator 	& string	& used classifier\\
features 		& list		& used features \\
preprocessing 	& list		& applied preprocessors in order\\
extra\_data 	& list 		& extra attributes passed to the first preprocessor \\
config			& dict		& extra configuration for all parts of pipeline \\
\bottomrule
\end{tabular}

\caption{Experiment Configuration}\label{tab:exp_dict}
\end{table}


The last section \texttt{graphs} is used for specifying what graphs will be plotted.
It is a list of individual figures.
Each figure is a dictionary of two elements; \texttt{name} and \texttt{data}.
Name specifies the filenames of the resulting graph ---
for each figure png and csv files will be created.
Element \texttt{data} specifies what evaluation metrics for what classifiers will be used.
It is a dictionary of experiment names, values being a list of all metrics to be output.

\autoref{tab:files} displays definitions of special files used for controlling the run of the software project.
Their exact usage is explained in appropriate sections.

\begin{table}[h]

\centering
\begin{tabular}{ll}
\toprule
\textbf{name of file}& \textbf{purpose} \\
\midrule
instance file		 & stores preprocessed instances \\
intermediate file	 & used for generating Geneea file \\
Geneea file			 & contains extra linguistics data for instances \\
experiments file	 & configures the experiments conducted \\
\bottomrule
\end{tabular}

\caption{File definition}\label{tab:files}
\end{table}

\todoA{add libraries used - as another appendix??}

\section{Special Files}

this probably should be moved to architecture

In \autoref{chap:arch}, we refer to special files.
Let us describe their exact format here.

\begin{table}[h]

\centering
\begin{tabular}{ll}
\toprule
\textbf{name of file}& \textbf{purpose} \\
\midrule
instance file		 & stores instances in the final format \\
intermediate file	 & used for generating Geneea file \\
Geneea file			 & contains extra linguistics data for instances \\
experiments file	 & configures the experiments conducted \\
\bottomrule
\end{tabular}

\caption{File definition}\label{tab:files}
\end{table}
There sbe on line-by-line correspondence between the Geneea file and the data file prepared locally.


