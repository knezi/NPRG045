\chapter{Text Classification}

% \begin{tikzpicture}
%   \begin{scope}[blend group = soft light]
%     \fill[red!30!white]   ( 90:1.2) circle (2);
%     \fill[green!30!white] (210:1.2) circle (2);
%     \fill[blue!30!white]  (330:1.2) circle (2);
%   \end{scope}
%   \node at ( 90:2)    {Typography};
%   \node at ( 210:2)   {Design};
%   \node at ( 330:2)   {Coding};
%   \node [font=\Large] {\LaTeX};
% \end{tikzpicture}

The process of~filtering data we described in the~first chapter is known as {\bf text classification}.
\citet{AggZhai12} describe text classification as follows.
We have a~set of~training records~$\mathcal{D} = \left\{X_1, \ldots, X_N\right\}$ and each record is labeled with one~of the~class labels indexed by~$\left\{1,\ldots, k\right\}$.
Every record consists of~attributes which contain some~information about it such as the text of a review or the number of stars given by the user.
We will refer to a~record as {\bf an~instance} to be consistent with machine learning terminology.

The~training set is used for building a~{\bf classification model}. Classification model is a~mapping between an~instance and a~class label.
In~this document we will take into account only \emph{hard version} of~classification which explicitly maps one and only one label to each record.  
Our data set contains classification of each instance, so our effort will be to build a classifier which in some respect makes the least mistakes when classifying our data set.
Later on we shall describe precisely what this means.
This approach of fitting a classifier to labelled data is called {\bf supervised learning}.

\section{Text Classification Process}

\todoA{diagram:}
\begin{code}
obtaining dataset
-> preprocessing {extraction} -> feature selection
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    		feature engineering

-> training -> evaluation
\end{code}
\begin{figure}
	blah
\label{cls_process}
\end{figure}

It is simple for us to obtain the data. We only download already prepared files from the Yelp Data Challenge.

In general, an instance is defined by its attributes.
However, we cannot feed these attributes directly to our classifier.
First, we have to process data into a suitable format. This process is called {\bf feature engineering}.
We will create the so called {\bf features}, which carry some information about the instance.
As we will see, it is hard to come up with features that would directly lead to successful classification.
Hence, we will try to generate as many features as we can and then try to select the most useful.
We will discuss both feature creation and selection in \autoref{chap:fea}.

When we have features ready, we can start training our classifiers. We will describe a general classifier in~\ref{chap:clsgen} and
individual classifiers in \autoref{chap:clscon}.

Finally, we want to be able to compare different classifiers and feature sets. Common evaluations will be discussed in \ref{chap:eval}.

Last three chapters are then left for the architecture of the software developed (\autoref{chap:arch}), demonstration of conducted experiments (\autoref{chap:exp}) and final discussion (\autoref{chap:concl}).


\section{Specification of Data with Terminology}

Let us define our task again with the~introduced terminology.
Each review is an~instance and file \texttt{review.json} contains an~instance per line.
Because the raw data cannot be directly used as features, we have to extract our features first.
An~example of such a~feature is the~number of words in a~review, which is calculated from the~review's text.
More on feature creation is discussed in \autoref{chap:fea}.
Business information will be only used indirectly to drop reviews coming for unreliable sources.


\section{What a classificator looks like}

\subsection{\todoA{hyperparameteres, parameters, model}}

what is it?

\subsection{discrm. vs generative}

\section{Classifiers}
\label{chap:clscon}

\subsection{Naive Bayes}
Probabilistic model for classification
Generally speaking, there are two models used for classification; generative and discriminative. Generative model assumes there exist 

Naive Bayes- WARN: document vs instance mismatch
Also this is only with word features
There are two classification algorithms commonly referred to as naive Bayes. However, we will discuss them at once, because the underlying idea is the same for both.
Naive Bayes uses generative model assuming each instance was generated by a parametric model. Because the model is not know, NB estimates the probability of an instance being generated by a class instead. We assume every document d\_i is generated by a mixture model parameterized by theta. The mixture model consists of components {c\_1, ...c\_n} component C\_i generating class c\_i. Thus a document is generated by first picking a model component (P[c\_i|theta]) and then being generated by that component P(P[d\_i|c\_i, Theta]).
Mathematically speaking, probability of d\_i being generated is:
P[d\_i | Theta] = sum over j = P[d\_i|c\_j, Theta] P[c\_j|Theta]


It  does so by maximum likelihood estimation (MLE).
MLE is a statistical method for estimating parameters such that observed (training instances in this case) is the most probable. For discrete distribution it equals to counting frequencies.
 
For classification one simply assesses the probability of each class to generate a classified instance and classify the instance asm the class which had the highest probability.

\subsection{Decision trees}
\todoC{Decision trees}

\subsection{FastText}


\subsection{\todoC{NN}}

\subsection{\todoC{Let you imagination go wild}}


\section{\todoB{interpretability -see w8}}
