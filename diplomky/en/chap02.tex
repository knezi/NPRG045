\chapter{Text Classification}

The process of~filtering data we described in the~first chapter is known as {\bf text classification}.
\citet{AggZhai12} describe text classification as follows.
We have a~set of~training records~$\mathcal{D} = \left\{X_1, \ldots, X_N\right\}$ and each record is labeled with one~of the~class labels indexed by~$\left\{1,\ldots, k\right\}$.
Every record consists of~attributes which contain some~information about it.

The~training set is used for building a~{\bf classification model}. Classification model is a~mapping between a~record and a~class label.
In~this document we will take into account only \emph{hard version} of~classification which explicitly maps one and only one label to each record.  


Subsequently, we have a~set of testing records.
These records are classified by our built models and used for comparing their performance.
More on exact evaluation in \autoref{chap:eval}.

To be consistent with machine learning terminology, we will refer to a~record as {\bf an~instance} and to an~attribute as {\bf a~feature}.

\section{Text Classification Process}

\todoA{diagram:}

text cat : a survey -norove - Aas

training set -\> preprocessing {extraction} -\> feature selection -\> 

\citet{Song14} 

\section{Thorough Specification of Data}

Let us define our task again with the~introduced terminology.
Each review is an~instance and file \texttt{review.json} contains an~instance per line.
Because the raw data cannot be directly used as features, we have to extract our features first.
An~example of such a~feature is the~number of words in a~review, which is calculated from the~review's text.
More on feature creation is discussed in \autoref{chap:fea}.
Business information will be only used indirectly to drop reviews coming for unreliable sources.

\section{Overview of the Classification Process}

As mentioned above, we will describe {\it feature engineering} in \autoref{chap:fea}.
Firstly, we will introduce several common approaches to creating features and also discuss possibilities of features tailored exactly for our problem.
Secondly, we will talk about {\it feature selection}, that is a process of identifying and eliminating unhelpful features for both classification and computational performance reasons.

The general concept of classifiers will be introduced in \autoref{chap:clsgen}, individual classifiers then in \autoref{chap:clscon}.
Common metrics used for evaluating performance of our classifier are discussed in \autoref{chap:eval}.

Last three chapters are then left for the architecture of the software developed (\autoref{chap:arch}), demonstration of conducted experiments (\autoref{chap:exp}) and final discussion (\autoref{chap:concl}).
