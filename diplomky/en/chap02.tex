\chapter{Text Classification}

The process of~filtering data we described in the~first chapter is known as {\bf text classification}.
\citet{AggZhai12} describe text classification as follows.
We have a~set of~training records~$\mathcal{D} = \left\{X_1, \ldots, X_N\right\}$ and each record is labeled with one~of the~class labels indexed by~$\left\{1,\ldots, k\right\}$.
Every record consists of~attributes which contain some~information about it.

The~training set is used for building a~{\bf classification model}. Classification model is a~mapping between a~record and a~class label.
In~this document we will take into account only \emph{hard version} of~classification which explicitly maps one and only one label to each record.  


Subsequently, we have a~set of testing records.
These records are classified by our built models and used for comparing their performance.
More on exact evaluation in \autoref{chap:eval}.

To be consistent with machine learning terminology, we will refer to a~record as {\bf an~instance} and to an~ attribute as {\bf a~feature}.

\section{Further specification of the data and the problem}

Let us define our task again with the introduced terminology. Each review is an instance and file \texttt{review.json} contains an instance per line. 

no features -> have to select them and that's it -> chapter engineering



\section{What to expect from the thesis}

\todoB[try them all - possible discussion on difficulty on different types].

All measurements will be with respect to the size of dataset, to see how the usefulness of approaches improves with more data.

In the first part, try to find best features. Start with single words chosen by different
criteria (TD-idf, entropy...), then try to add *-grams. Later, try more linguistics features.

In the next part, if you have time, try other approaches to improve performance. You can try PCA reduction, NN, or other ML algorithms/techniques.

\section{OVERVIEW of the goals}:

\subsection{First try classification with ngrams only:}

\bf Words \rm

 \todoA{- TD*IDF - with given threshholds}

 \todoB{try TD*IDF not with a given threshold, but rather find te optimal threshold for cuts}

 \todoB{- choose words with highest information gain; as in McCallum Event models}

 \todoA{Fasttext}

\bf Groups of words: \rm

\bf entities extracted from geneea \rm

 \todoB{- filter them based on the geneea value of importance}

 \todoA{- occurrences - pick only the most frequent}

\bf simple Bigrams, moregrams\rm

\todoA{- same approach as for unigrams}

\subsection{Later try adding more features (mainly linguistics):}

\todoB{Cosine sim}

\todoA{Spell check}

\todoA{Review length}

\todoB{Stars as non-linguistic feature}

\todoC{Go your imagination wild}

\subsection{Try non-feature improvements:}

\todoB{PCA/SVD reduction}

\todoC{Decision trees}

\todoC{NN}

\todoC{Let you imagination go wild}
